{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2018 16:49:10 - INFO - allennlp.commands.elmo -   Initializing ELMo.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.load('./coref_elmo_dir/best.th')\n",
    "# print(type(a))\n",
    "\n",
    "# print(sorted([len(a[i]) for i in a.keys()]))\n",
    "\n",
    "# print([i for i in a.keys() if len(a[i]) == 1024])\n",
    "# # print([i for i in a.keys() if len(a[i]) == 1])\n",
    "\n",
    "names = ['_text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma', '_text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0', '_text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1', '_text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2']\n",
    "\n",
    "weights = [float(a[i]) for i in names]\n",
    "\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = elmo.embed_sentence([\"Richard\", \"loves\", \"himself\", \"so\", \"much\", \"that\", \"he\", \"made\", \"love\", \"to\", \"himself\"])\n",
    "# orig_sentence = \"dogs ran around the yard . they ran quickly . bananas taste good .\".split(\" \")\n",
    "\n",
    "orig_sentence = [\"In\", \"the\", \"same\", \"way\", \"that\", \"no\", \"country\", \"has\", \"a\", \"law\", \"against\", \"cannibals\", \"eating\", \"its\", \"prime\", \"minister\", \",\", \"because\", \"such\", \"an\", \"act\", \"is\", \"unthinkable\", \",\", \"international\", \"law\", \"does\", \"not\", \"address\", \"killers\", \"shooting\", \"from\", \"hospitals\", \",\", \"mosques\", \"and\", \"ambulances\", \",\", \"while\", \"being\", \"protected\", \"by\", \"their\", \"Government\", \"or\", \"society\", \".\", \"International\", \"law\", \"does\", \"not\", \"know\", \"how\", \"to\", \"handle\", \"someone\", \"who\", \"sends\", \"children\", \"to\", \"throw\", \"stones\", \",\", \"stands\", \"behind\", \"them\", \"and\", \"shoots\", \"with\", \"immunity\", \"and\", \"can\", \"not\", \"be\", \"arrested\", \"because\", \"he\", \"is\", \"sheltered\", \"by\", \"a\", \"Government\", \".\", \"International\", \"law\", \"does\", \"not\", \"know\", \"how\", \"to\", \"deal\", \"with\", \"a\", \"leader\", \"of\", \"murderers\", \"who\", \"is\", \"royally\", \"and\", \"comfortably\", \"hosted\", \"by\", \"a\", \"country\", \",\", \"which\", \"pretends\", \"to\", \"condemn\", \"his\", \"acts\", \"or\", \"just\", \"claims\", \"to\", \"be\", \"too\", \"weak\", \"to\", \"arrest\", \"him\", \".\"]\n",
    "vectors = elmo.embed_sentence(orig_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7737881392240524\n",
      "0.8022511005401611\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "val1 = weights[0]*np.add(np.add(weights[1]*vectors[0][1], weights[2]*vectors[1][1]), weights[3]*vectors[2][1])\n",
    "\n",
    "val2 = weights[0]*np.add(np.add(weights[1]*vectors[0][7], weights[2]*vectors[1][7]), weights[3]*vectors[2][7])\n",
    "\n",
    "dist = scipy.spatial.distance.cosine(val1, val2) # cosine distance between \"apple\" and \"carrot\" in the last layer\n",
    "dist2 = scipy.spatial.distance.cosine(vectors[2][1], vectors[2][7])\n",
    "print(dist)\n",
    "print(dist2)\n",
    "print(len(vectors[0]))\n",
    "# 0.18020617961883545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, 35, ',', 'and', 0.27203869819641113), (24, 47, 'international', 'International', 0.27933722734451294), (24, 83, 'international', 'International', 0.32194018363952637), (32, 34, 'hospitals', 'mosques', 0.34638386964797974), (98, 100, 'royally', 'comfortably', 0.3491635322570801)]\n",
      "[(98, 100, 'royally', 'comfortably', 0.20597773790359497), (32, 34, 'hospitals', 'mosques', 0.20960217714309692), (16, 17, ',', 'because', 0.23645401000976562), (24, 47, 'international', 'International', 0.24221879243850708), (40, 78, 'protected', 'sheltered', 0.24946069717407227)]\n"
     ]
    }
   ],
   "source": [
    "elmo_embeddings = {}\n",
    "reg_embeddings = {}\n",
    "for i in range(len(vectors[0])):\n",
    "    embedding = weights[0]*np.add(np.add(weights[1]*vectors[0][i], weights[2]*vectors[1][i]), weights[3]*vectors[2][i])\n",
    "    elmo_embeddings[orig_sentence[i] + \" \" + str(i)] = (i, embedding)\n",
    "    reg_embeddings[orig_sentence[i] + \" \" + str(i)] = (i, vectors[2][i])\n",
    "\n",
    "    \n",
    "def similarities(embedding_dict):\n",
    "    cosine_sims = []\n",
    "    for w1, (i, e1) in embedding_dict.items():\n",
    "        for w2, (j, e2) in embedding_dict.items():\n",
    "            if j <= i or w2.split()[0] == w1.split()[0]: \n",
    "                continue\n",
    "            dist = scipy.spatial.distance.cosine(e1,e2)\n",
    "            cosine_sims.append((i,j,w1.split()[0],w2.split()[0],dist))\n",
    "    cosine_sims.sort(key=lambda x: x[4], reverse = False)\n",
    "#     print(cosine_sims)\n",
    "    return cosine_sims\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "sim_elmo = similarities(elmo_embeddings)[:top_n]\n",
    "sim_reg = similarities(reg_embeddings)[:top_n]\n",
    "\n",
    "print(sim_elmo)\n",
    "\n",
    "# print()\n",
    "\n",
    "print(sim_reg)\n",
    "\n",
    "# print(len(set([(a, b, c, d) for a, b, c, d, e in sim_elmo]).intersection(set([(a, b, c, d) for a, b, c, d, e in sim_reg]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_elmo_dict = {}\n",
    "for i,j,w1,w2,dist in sim_elmo:\n",
    "    sim_elmo_dict[str(i)+\" \"+str(j)] = (w1,w2,dist)\n",
    "\n",
    "sim_reg_dict = {}\n",
    "for i,j,w1,w2,dist in sim_reg:\n",
    "    sim_reg_dict[str(i)+\" \"+str(j)] = (w1,w2,dist)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'33 35'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c4b42c4a98df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msim_elmo_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msim_elmo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msim_reg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#         pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elmo <= Regular\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_elmo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim_reg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '33 35'"
     ]
    }
   ],
   "source": [
    "for k,v in sim_elmo_dict.items():\n",
    "    if (sim_elmo_dict[k][2] <= sim_reg_dict[k][2]):\n",
    "#         pass\n",
    "        print(\"Elmo <= Regular\")\n",
    "        print(sim_elmo_dict[k],sim_reg_dict[k])\n",
    "    if (sim_elmo_dict[k][2] > sim_reg_dict[k][2]):\n",
    "        pass\n",
    "#         print(\"Elmo > Regular\")\n",
    "#         print(sim_elmo_dict[k],sim_reg_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.016251564025879,\n",
       " 0.027123697102069855,\n",
       " 0.038384225219488144,\n",
       " -0.03157595172524452]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting CUDA_VISIBLE_DEVICES to: \n",
      "Running experiment: trial\n",
      "max_top_antecedents = 50\n",
      "max_training_sentences = 50\n",
      "top_span_ratio = 0.4\n",
      "filter_widths = [\n",
      "  3\n",
      "  4\n",
      "  5\n",
      "]\n",
      "filter_size = 50\n",
      "char_embedding_size = 8\n",
      "char_vocab_path = \"char_vocab.english.txt\"\n",
      "context_embeddings {\n",
      "  path = \"glove.840B.300d.txt\"\n",
      "  size = 300\n",
      "}\n",
      "head_embeddings {\n",
      "  path = \"glove_50_300_2.txt\"\n",
      "  size = 300\n",
      "}\n",
      "contextualization_size = 200\n",
      "contextualization_layers = 3\n",
      "ffnn_size = 150\n",
      "ffnn_depth = 2\n",
      "feature_size = 20\n",
      "max_span_width = 30\n",
      "use_metadata = true\n",
      "use_features = true\n",
      "model_heads = true\n",
      "coref_depth = 2\n",
      "lm_layers = 3\n",
      "lm_size = 1024\n",
      "coarse_to_fine = true\n",
      "max_gradient_norm = 5.0\n",
      "lstm_dropout_rate = 0.4\n",
      "lexical_dropout_rate = 0.5\n",
      "dropout_rate = 0.2\n",
      "optimizer = \"adam\"\n",
      "learning_rate = 0.001\n",
      "decay_rate = 0.999\n",
      "decay_frequency = 100\n",
      "train_path = \"train.english.jsonlines\"\n",
      "eval_path = \"dev.english.jsonlines\"\n",
      "conll_eval_path = \"dev.english.conll\"\n",
      "lm_path = \"\"\n",
      "genres = [\n",
      "  \"bc\"\n",
      "  \"bn\"\n",
      "  \"mz\"\n",
      "  \"nw\"\n",
      "  \"pt\"\n",
      "  \"tc\"\n",
      "  \"wb\"\n",
      "]\n",
      "eval_frequency = 1\n",
      "report_frequency = 1\n",
      "log_root = \"logs\"\n",
      "cluster {\n",
      "  addresses {\n",
      "    ps = [\n",
      "      \"localhost:2222\"\n",
      "    ]\n",
      "    worker = [\n",
      "      \"localhost:2223\"\n",
      "      \"localhost:2224\"\n",
      "    ]\n",
      "  }\n",
      "  gpus = [\n",
      "    0\n",
      "    1\n",
      "  ]\n",
      "}\n",
      "log_dir = \"logs/trial\"\n",
      "300 hhh\n",
      "Loading word embeddings from glove.840B.300d.txt...\n",
      ",\n",
      ".\n",
      "the\n",
      "and\n",
      "to\n",
      "of\n",
      "a\n",
      "in\n",
      "\"\n",
      ":\n",
      "is\n",
      "for\n",
      "I\n",
      ")\n",
      "(\n",
      "that\n",
      "-\n",
      "on\n",
      "you\n",
      "with\n",
      "'s\n",
      "it\n",
      "The\n",
      "are\n",
      "by\n",
      "at\n",
      "be\n",
      "this\n",
      "as\n",
      "from\n",
      "was\n",
      "have\n",
      "or\n",
      "...\n",
      "your\n",
      "not\n",
      "!\n",
      "?\n",
      "will\n",
      "an\n",
      "n't\n",
      "can\n",
      "but\n",
      "all\n",
      "my\n",
      "has\n",
      "|\n",
      "do\n",
      "we\n",
      "they\n",
      "more\n",
      "one\n",
      "about\n",
      "he\n",
      ";\n",
      "'\n",
      "out\n",
      "$\n",
      "their\n",
      "so\n",
      "his\n",
      "up\n",
      "It\n",
      "&\n",
      "like\n",
      "/\n",
      "1\n",
      "which\n",
      "if\n",
      "would\n",
      "our\n",
      "[\n",
      "]\n",
      "me\n",
      "who\n",
      "just\n",
      "This\n",
      "time\n",
      "what\n",
      "A\n",
      "2\n",
      "had\n",
      "when\n",
      "there\n",
      "been\n",
      "some\n",
      "get\n",
      "were\n",
      "other\n",
      "also\n",
      "In\n",
      "her\n",
      "them\n",
      "You\n",
      "new\n",
      "We\n",
      "no\n",
      "any\n",
      ">\n",
      "people\n",
      "Done loading word embeddings.\n",
      "300 hhh\n",
      "Loading word embeddings from glove_50_300_2.txt...\n",
      ",\n",
      "the\n",
      ".\n",
      "to\n",
      "of\n",
      "and\n",
      "a\n",
      "in\n",
      "'s\n",
      "``\n",
      "''\n",
      "that\n",
      "for\n",
      "said\n",
      "on\n",
      "The\n",
      "is\n",
      "was\n",
      "with\n",
      "(\n",
      ")\n",
      "at\n",
      "by\n",
      "as\n",
      "from\n",
      "he\n",
      "it\n",
      "be\n",
      "have\n",
      "has\n",
      "his\n",
      "are\n",
      "--\n",
      "an\n",
      "not\n",
      "will\n",
      ":\n",
      "I\n",
      "who\n",
      "were\n",
      "had\n",
      "their\n",
      ";\n",
      "they\n",
      "but\n",
      "its\n",
      "would\n",
      "this\n",
      "which\n",
      "been\n",
      "or\n",
      "more\n",
      "after\n",
      "about\n",
      "_\n",
      "percent\n",
      "up\n",
      "n't\n",
      "one\n",
      "two\n",
      "also\n",
      "It\n",
      "out\n",
      "year\n",
      "$\n",
      "than\n",
      "people\n",
      "He\n",
      "But\n",
      "first\n",
      "last\n",
      "we\n",
      "government\n",
      "when\n",
      "In\n",
      "over\n",
      "'\n",
      "all\n",
      "other\n",
      "A\n",
      "into\n",
      "new\n",
      "you\n",
      "million\n",
      "can\n",
      "do\n",
      "her\n",
      "years\n",
      "U.S.\n",
      "could\n",
      "some\n",
      "time\n",
      "there\n",
      "New\n",
      "We\n",
      "no\n",
      "against\n",
      "she\n",
      "them\n",
      "United\n",
      "103 63\n",
      "Done loading word embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Analyze the glove vectors\n",
    "import numpy as np\n",
    "import collections\n",
    "import util\n",
    "import sys\n",
    "sys.argv[1] = 'trial'\n",
    "config = util.initialize_from_env()\n",
    "context_embeddings = util.EmbeddingDictionary(\n",
    "    config[\"context_embeddings\"])\n",
    "head_embeddings = util.EmbeddingDictionary(\n",
    "    config[\"head_embeddings\"], maybe_cache=context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'enumerate' object has no attribute '_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0c8d9faf24ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcosine_sims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhead_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'enumerate' object has no attribute '_embeddings'"
     ]
    }
   ],
   "source": [
    "cosine_sims = []\n",
    "head_embeddings = enumerate(head_embeddings._embeddings.items())\n",
    "for i, (w1, e1) in head_embeddings:\n",
    "    for j, (w2, e2) in head_embeddings:\n",
    "        if i < j:\n",
    "            dist = scipy.spatial.distance.cosine(e1,e2)\n",
    "            cosine_sims.append((w1,w2,dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_dict = {}\n",
    "for w1,w2,dist in cosine_sims:\n",
    "    cosine_sim_dict[w1 + \" \" + w2] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", because 0.45479732751846313\n"
     ]
    }
   ],
   "source": [
    "for i,j,w1,w2,dist in sim_reg:\n",
    "    w1 = w1.lower()\n",
    "    w2 = w2.lower()\n",
    "    if w1 + ' ' + w2 in cosine_sim_dict:\n",
    "        print(w1 + ' ' + w2, cosine_sim_dict[w1 + ' ' + w2])\n",
    "    elif w2 + ' ' + w1 in cosine_sim_dict:\n",
    "        print(w1 + ' ' + w2, cosine_sim_dict[w2 + ' ' + w1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'international', ',', 'and', 'International', 'because'}\n"
     ]
    }
   ],
   "source": [
    "words_elmo = []\n",
    "words_dict = []\n",
    "for i, j, w1, w2, _ in sim_elmo + sim_reg:\n",
    "    words_elmo.append(w1)\n",
    "    words_elmo.append(w2)\n",
    "    \n",
    "for key in cosine_sim_dict:\n",
    "    k = key.split(\" \")\n",
    "#     print(k)\n",
    "    words_dict.append(k[0])\n",
    "    words_dict.append(k[1])\n",
    "    \n",
    "print((set(words_elmo).intersection(set(words_dict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
